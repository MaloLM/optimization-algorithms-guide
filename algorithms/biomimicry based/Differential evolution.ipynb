{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6d5ef08",
   "metadata": {},
   "source": [
    "# Differential evolution\n",
    "\n",
    "![](../../assets/images/d_ev_0.jpg \"Dall-E Generation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abfa78c",
   "metadata": {},
   "source": [
    "## Understanding the algorithm\n",
    "\n",
    "Differential Evolution (DE) is an evolutionary algorithm primarily used for optimization problems, particularly in continuous domains. Developed by Storn and Price between 1995 and 1997, it is recognized for its simplicity and effective problem-solving capabilities, making it a popular choice among population-based algorithms. DE operates as a stochastic algorithm, meaning it incorporates randomness into its process, and is particularly effective for numerical continuous optimization challenges.\n",
    "\n",
    "The DE algorithm consists of 4 main steps:\n",
    "\n",
    "1. **Initialization**: This involves creating an initial population of candidate solutions, typically represented as vectors of real values. These values correspond to potential solutions in the search space of the optimization problem.\n",
    "\n",
    "2. **Mutation**: In this step, the algorithm modifies the existing population members to explore the solution space. This is done by combining the attributes of different population members in specific ways, leading to new candidate solutions.\n",
    "\n",
    "3. **Crossover**: Also known as recombination, this step involves combining the mutated solutions with existing ones to create a new generation of potential solutions. The crossover mechanism is crucial for maintaining diversity in the population and for allowing the algorithm to explore new areas of the solution space.\n",
    "\n",
    "4. **Selection**: Finally, the selection process determines which candidate solutions are carried over to the next generation. This is usually based on how well they solve the optimization problem, with better-performing solutions being more likely to be selected.\n",
    "\n",
    "A key aspect of DE is its flexibility and adaptability, allowing it to be applied to a wide range of optimization problems. As a metaheuristic algorithm, it makes few assumptions about the problem being optimized and is capable of exploring vast design spaces efficiently. This characteristic makes DE suitable for problems where the solution space is large or poorly understood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12c7314-7ccd-46de-95a4-0a22010513ae",
   "metadata": {},
   "source": [
    "## Usage examples\n",
    "\n",
    "1. **Parallel Computing**: DE has been utilized effectively in parallel computing environments. It's particularly useful for optimizing problems that are computationally intensive and can benefit from parallel execution.\n",
    "\n",
    "2. **Multiobjective Optimization**: DE is applied in multiobjective optimization scenarios, where it helps in finding solutions that balance trade-offs between two or more conflicting objectives.\n",
    "\n",
    "3. **Constrained Optimization**: In constrained optimization, DE helps in finding the best solution within a given set of constraints, making it suitable for real-world problems where certain limits or requirements must be met.\n",
    "\n",
    "4. **Quantitative Interpretation of Self-Potential Data in Geophysics**: DE has been successfully used for the quantitative interpretation of self-potential data in geophysics. This involves estimating parameters such as electrical dipole moment, depth of the source, and polarization angle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7767bb7",
   "metadata": {},
   "source": [
    "## Strengths\n",
    "\n",
    "1. **Handling Non-Differentiable, Nonlinear, and Multimodal Functions**: DE excels in managing non-differentiable, nonlinear, and multimodal cost functions. This capability is crucial when dealing with real-world optimization problems that are complex and don't conform to simpler, linear models.\n",
    "\n",
    "2. **Parallelizability**: The algorithm is well-suited for parallel computation, making it effective for computationally intensive tasks. This aspect allows for efficient processing of large-scale problems by distributing the workload across multiple processors or machines.\n",
    "\n",
    "3. **Ease of Use with Few Control Variables**: DE is user-friendly, requiring only a few control variables to guide the optimization process. These variables are robust and straightforward to select, reducing the complexity often associated with tuning optimization algorithms.\n",
    "\n",
    "4. **Good Convergence Properties**: It consistently converges to the global minimum in independent trials, indicating its reliability in finding optimal solutions across different problem instances.\n",
    "\n",
    "5. **Capability with Multidimensional Real-Valued Functions**: DE can optimize multidimensional real-valued functions without the need for gradient information. This means it can address optimization problems that are non-continuous, noisy, or subject to change over time, broadening its applicability.\n",
    "\n",
    "6. **Incorporates Directional Information**: Unlike conventional genetic algorithms, DE employs a target and unit vector, which allows for quicker convergence on solutions. However, this may come at the expense of exploration.\n",
    "\n",
    "7. **Simplicity and Speed**: DE is valued for its straightforwardness and rapid execution. It employs real coding, which is easy to understand and implement, and also features a local search mechanism for enhanced efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfb1301",
   "metadata": {},
   "source": [
    "## Weaknesses\n",
    "\n",
    "1. **Parameter Sensitivity and Selection**: The performance of DE is highly dependent on the trial vector generation scheme and the adjustment of its control parameters. Finding the optimal values for these parameters can be time-consuming and challenging, especially for complex problems. This sensitivity necessitates a problem-specific approach to parameter selection, which can be a significant drawback in applications where quick and easy configuration is needed.\n",
    "\n",
    "2. **Effort in Fine-Tuning Parameters**: The mutation factor (F) and crossover probability (CR) are critical to DE's performance. The trial-and-error method used to fine-tune these parameters can be successful, but it demands considerable time and effort. Moreover, the optimal settings for these parameters tend to be problem-specific, adding to the complexity of using DE in diverse scenarios.\n",
    "\n",
    "3. **Selection of Penalty Coefficient**: In DE, the selection of the penalty coefficient is a significant challenge. If the penalty coefficient is set too low, it may not effectively enforce constraints. Conversely, if it's set too high, it can greatly slow down or even halt the convergence process. This aspect is crucial because it impacts the effectiveness of the algorithm in constrained optimization problems.\n",
    "\n",
    "4. **Premature Convergence and Evolutionary Stagnation**: As the DE algorithm evolves, with an increase in generations, the population diversity can worsen. This leads to premature convergence or evolutionary stagnation, which is detrimental for an algorithm that relies on population differences. This weakness is particularly problematic as it impacts the algorithm's ability to find optimal solutions in diverse scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ede478d",
   "metadata": {},
   "source": [
    "## Python demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c418c639-2bfa-462a-ad4c-f3c426a8ac0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of demonstration\n",
    "\n",
    "---\n",
    "\n",
    "## Practical optimization tools\n",
    "\n",
    "2. [**DEAP (Python):**](https://github.com/DEAP/deap) DEAP (Distributed Evolutionary Algorithms in Python) is an open-source framework for evolutionary computation. It provides a versatile collection of evolutionary algorithms, including differential evolution. DEAP stands out for its ease of use and flexibility, allowing users to easily customize genetic algorithms and other evolutionary strategies for their specific problems.\n",
    "\n",
    "3. [**Julia (Julia Language):**](https://julialang.org/) Julia, a high-performance programming language for technical computing, includes various packages for optimization and evolutionary algorithms. One such package is `BlackBoxOptim`, which supports differential evolution. Julia is renowned for its speed and efficiency, particularly in mathematical computing, making it a popular choice for implementing complex algorithms like differential evolution.\n",
    "\n",
    "4. [**Nevergrad (Python):**](https://github.com/facebookresearch/nevergrad) Nevergrad is an open-source Python library designed for derivative-free and blackbox optimization. It includes a wide range of optimization algorithms, including differential evolution. Known for its ability to handle noisy, real-world problems, Nevergrad is widely used in both academia and industry for optimization tasks that are difficult to model explicitly.\n",
    "\n",
    "5. [**Jenetics (Java):**](https://jenetics.io/) Jenetics is an advanced Genetic Algorithm, respectively an Evolutionary Algorithm, library written in modern-day Java. It includes support for differential evolution as part of its suite of evolutionary algorithms. Jenetics is appreciated for its robustness and performance, making it a strong choice for Java developers needing evolutionary computation capabilities.\n",
    "\n",
    "5. [**Pagmo/Pygmo (C++/Python):**](https://www.esa.int/gsp/ACT/open_source/pagmo/) Pagmo is a C++ library for scientific computing and optimization, particularly focused on global and multi-objective optimization problems. It includes the differential evolution algorithm among many other optimization algorithms. Pygmo is the Python version of this library. Pagmo is distinguished by its architecture that facilitates solving complex optimization problems and its integration with other scientific tools in C++ and Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "| Sources |\n",
    "|---------|\n",
    "| [Differential evolution - Wikipedia](https://en.wikipedia.org/wiki/Differential_evolution) |\n",
    "| [Differential Evolution from Scratch in Python - Machinelearningmastery](https://machinelearningmastery.com/differential-evolution-from-scratch-in-python/) |\n",
    "| [Exploring differential evolution in AI - indiaai](https://indiaai.gov.in/article/exploring-differential-evolution-in-ai) |\n",
    "| [A Comparative Study of Differential Evolution Variants in Constrained Structural Optimization - Frontiers](https://www.frontiersin.org/articles/10.3389/fbuil.2020.00102/full) |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
