{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6d5ef08",
   "metadata": {},
   "source": [
    "# Differential evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abfa78c",
   "metadata": {},
   "source": [
    "## Understanding the algorithm\n",
    "\n",
    "Differential Evolution (DE) is an evolutionary algorithm primarily used for optimization problems, particularly in continuous domains. Developed by Storn and Price between 1995 and 1997, it is recognized for its simplicity and effective problem-solving capabilities, making it a popular choice among population-based algorithms. DE operates as a stochastic algorithm, meaning it incorporates randomness into its process, and is particularly effective for numerical continuous optimization challenges.\n",
    "\n",
    "The DE algorithm consists of 4 main steps:\n",
    "\n",
    "1. **Initialization**: This involves creating an initial population of candidate solutions, typically represented as vectors of real values. These values correspond to potential solutions in the search space of the optimization problem.\n",
    "\n",
    "2. **Mutation**: In this step, the algorithm modifies the existing population members to explore the solution space. This is done by combining the attributes of different population members in specific ways, leading to new candidate solutions.\n",
    "\n",
    "3. **Crossover**: Also known as recombination, this step involves combining the mutated solutions with existing ones to create a new generation of potential solutions. The crossover mechanism is crucial for maintaining diversity in the population and for allowing the algorithm to explore new areas of the solution space.\n",
    "\n",
    "4. **Selection**: Finally, the selection process determines which candidate solutions are carried over to the next generation. This is usually based on how well they solve the optimization problem, with better-performing solutions being more likely to be selected.\n",
    "\n",
    "A key aspect of DE is its flexibility and adaptability, allowing it to be applied to a wide range of optimization problems. As a metaheuristic algorithm, it makes few assumptions about the problem being optimized and is capable of exploring vast design spaces efficiently. This characteristic makes DE suitable for problems where the solution space is large or poorly understood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12c7314-7ccd-46de-95a4-0a22010513ae",
   "metadata": {},
   "source": [
    "## Usage examples\n",
    "\n",
    "1. **Optimisation de Paramètres en Machine Learning** : L'Évolution Différentielle est utilisée pour optimiser les hyperparamètres dans les modèles de machine learning, tels que les réseaux de neurones. Sa capacité à naviguer dans des espaces de recherche complexes et à gérer des paramètres continus la rend idéale pour affiner la performance des modèles.\n",
    "\n",
    "2. **Conception d'Antennes dans l'Ingénierie RF** : Dans la conception d'antennes et d'autres structures en génie RF (radiofréquence), l'ED est utilisé pour optimiser la forme et la taille des composants pour atteindre les meilleures performances. Sa robustesse face aux problèmes d'optimisation multicritères et non linéaires est particulièrement utile ici.\n",
    "\n",
    "3. **Optimisation de Processus Chimiques** : L'Évolution Différentielle aide à optimiser les processus dans l'industrie chimique, comme la maximisation du rendement ou la minimisation de la consommation d'énergie. Grâce à sa capacité à gérer des espaces de solutions complexes, l'ED est efficace pour trouver des configurations optimales dans des processus multiparamétriques.\n",
    "\n",
    "4. **Gestion de l'Énergie dans les Réseaux Électriques** : L'algorithme est appliqué dans la gestion et l'optimisation de l'énergie dans les réseaux électriques, notamment pour l'optimisation de la distribution de charge et la planification de la production d'énergie. L'ED est capable de traiter efficacement les contraintes et les objectifs multiples caractéristiques de ces systèmes.\n",
    "\n",
    "5. **Optimisation de Portefeuille en Finance** : Dans le domaine financier, l'Évolution Différentielle est utilisée pour l'optimisation de portefeuille, où l'objectif est de maximiser le rendement tout en minimisant le risque. L'algorithme est bien adapté pour explorer de manière efficace l'espace de solutions diversifié et complexe typique des marchés financiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7767bb7",
   "metadata": {},
   "source": [
    "## Strengths\n",
    "\n",
    "1. **Robustesse dans des Espaces de Recherche Complexes** : L'Évolution Différentielle excelle dans des environnements où les espaces de recherche sont non-linéaires, multimodaux ou discontinus. Sa capacité à explorer de manière efficace ces espaces complexes en fait un choix robuste pour de nombreux problèmes d'optimisation.\n",
    "\n",
    "2. **Peu de Paramètres à Régler** : Contrairement à d'autres algorithmes d'optimisation, l'ED nécessite un réglage minimal des paramètres, ce qui le rend plus accessible et plus facile à utiliser, en particulier pour les non-spécialistes.\n",
    "\n",
    "3. **Bonne Capacité d'Exploration et d'Exploitation** : L'algorithme maintient un équilibre entre l'exploration de nouvelles zones de l'espace de recherche et l'exploitation des solutions prometteuses trouvées. Cela permet à l'ED d'éviter de rester piégé dans des optima locaux.\n",
    "\n",
    "4. **Adaptabilité à Différents Types de Problèmes** : L'Évolution Différentielle peut être appliquée à une grande variété de problèmes, qu'ils soient continus ou discrets, et s'adapte bien aux problèmes d'optimisation multicritères.\n",
    "\n",
    "5. **Convergence Rapide** : L'algorithme a tendance à converger vers une solution optimale ou quasi-optimale plus rapidement que d'autres méthodes, en particulier dans des situations où les autres algorithmes d'optimisation échouent ou sont inefficaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfb1301",
   "metadata": {},
   "source": [
    "## Weaknesses\n",
    "\n",
    "1. **Risque de Convergence Prématurée** : Bien que l'Évolution Différentielle soit efficace dans de nombreux cas, elle peut parfois converger prématurément vers des optima locaux, en particulier dans des espaces de recherche extrêmement complexes ou multimodaux.\n",
    "\n",
    "2. **Sensibilité aux Paramètres** : Bien que l'ED nécessite moins de réglages de paramètres par rapport à d'autres algorithmes, la performance peut être sensible à la configuration de ces paramètres, comme la taille de la population ou les facteurs de mutation et de recombinaison.\n",
    "\n",
    "3. **Coût Computationnel sur de Grands Espaces de Recherche** : Pour des problèmes avec de très grandes dimensions ou des espaces de recherche étendus, l'Évolution Différentielle peut être coûteuse en termes de temps de calcul et de ressources, particulièrement si une grande population est nécessaire.\n",
    "\n",
    "4. **Pas Toujours Idéal pour les Problèmes Discrets** : L'ED est principalement conçu pour les problèmes d'optimisation continue. Bien qu'il puisse être adapté pour des problèmes discrets, il peut être moins efficace comparé à d'autres algorithmes spécifiquement conçus pour ces types de problèmes.\n",
    "\n",
    "5. **Manque de Garanties Théoriques** : Contrairement à certains algorithmes d'optimisation, l'Évolution Différentielle ne possède pas toujours de garanties théoriques fortes quant à la convergence ou l'optimalité, ce qui peut être un inconvénient pour des applications nécessitant des garanties rigoureuses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ede478d",
   "metadata": {},
   "source": [
    "## Python demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c418c639-2bfa-462a-ad4c-f3c426a8ac0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of demonstration\n",
    "\n",
    "---\n",
    "\n",
    "## Practical optimization tools\n",
    "\n",
    "2. [**DEAP (Python):**](https://github.com/DEAP/deap) DEAP (Distributed Evolutionary Algorithms in Python) is an open-source framework for evolutionary computation. It provides a versatile collection of evolutionary algorithms, including differential evolution. DEAP stands out for its ease of use and flexibility, allowing users to easily customize genetic algorithms and other evolutionary strategies for their specific problems.\n",
    "\n",
    "3. [**Julia (Julia Language):**](https://julialang.org/) Julia, a high-performance programming language for technical computing, includes various packages for optimization and evolutionary algorithms. One such package is `BlackBoxOptim`, which supports differential evolution. Julia is renowned for its speed and efficiency, particularly in mathematical computing, making it a popular choice for implementing complex algorithms like differential evolution.\n",
    "\n",
    "4. [**Nevergrad (Python):**](https://github.com/facebookresearch/nevergrad) Nevergrad is an open-source Python library designed for derivative-free and blackbox optimization. It includes a wide range of optimization algorithms, including differential evolution. Known for its ability to handle noisy, real-world problems, Nevergrad is widely used in both academia and industry for optimization tasks that are difficult to model explicitly.\n",
    "\n",
    "5. [**Jenetics (Java):**](https://jenetics.io/) Jenetics is an advanced Genetic Algorithm, respectively an Evolutionary Algorithm, library written in modern-day Java. It includes support for differential evolution as part of its suite of evolutionary algorithms. Jenetics is appreciated for its robustness and performance, making it a strong choice for Java developers needing evolutionary computation capabilities.\n",
    "\n",
    "5. [**Pagmo/Pygmo (C++/Python):**](https://www.esa.int/gsp/ACT/open_source/pagmo/) Pagmo is a C++ library for scientific computing and optimization, particularly focused on global and multi-objective optimization problems. It includes the differential evolution algorithm among many other optimization algorithms. Pygmo is the Python version of this library. Pagmo is distinguished by its architecture that facilitates solving complex optimization problems and its integration with other scientific tools in C++ and Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "| Sources |\n",
    "|---------|\n",
    "| [Ant colony optimization algorithms - Wikipedia](https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms) |\n",
    "| [Algorithme de colonie de fourmis - 123dok.net](https://123dok.net/article/algorithme-colonie-fourmis-m%C3%A9thodes-r%C3%A9solution.zln9ol6q) |\n",
    "| [Algorithme de colonies de fourmis - Apprendre-en-ligne.com](https://www.apprendre-en-ligne.net/info/algo/fourmis.html) |\n",
    "| [Colonie de fourmis - complex-systems-ai.com](https://complex-systems-ai.com/algorithmes-dessaims/colonie-de-fourmis/) |\n",
    "| [Algorithme de colonies de fourmis - fr.wikipedia](https://fr.wikipedia.org/wiki/Algorithme_de_colonies_de_fourmis) |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
