{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6d5ef08",
   "metadata": {},
   "source": [
    "# Particle swarm optimisation (PSO)\n",
    "\n",
    "![](../../assets/images/pso_0.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abfa78c",
   "metadata": {},
   "source": [
    "## Understanding the algorithm\n",
    "\n",
    "Particle Swarm Optimization (PSO) is a computational technique inspired by the social behavior of natural organisms, such as birds or fish, which move together to achieve a common goal. In PSO, a group of particles, each representing a potential solution, navigates through a problemâ€™s solution space to find the best possible solution. This approach is particularly effective in scenarios where a mathematical function's global maximum or minimum is challenging to determine due to multiple variables or higher-dimensional vector space.\n",
    "\n",
    "The essential components of PSO include the following:\n",
    "\n",
    "1. **Objective Function (f):** This is the function that needs to be optimized, either minimized or maximized, depending on the problem.\n",
    "\n",
    "2. **Particles:** These are agents within the PSO that move through the solution space. Each particle has a position and velocity, and they are initially distributed randomly.\n",
    "\n",
    "3. **Personal Best (Pb) and Global Best (gb):** Each particle keeps track of its best position so far (Pb) and the overall best position found by any particle (gb).\n",
    "\n",
    "4. **Velocity Update:** The movement of particles is guided by their velocity, which is adjusted based on their own experience (Pb) and the experience of the entire group (gb). This adjustment uses certain parameters like inertia weight (W), cognitive constant (C1), and social constant (C2), along with random factors to ensure diverse exploration of the solution space.\n",
    "\n",
    "5. **Position Update:** After velocity is updated, particles move to new positions. This process is iterative, with the objective function re-evaluated at each new position.\n",
    "\n",
    "![concept of particle swarm optimization. It features two parts, showing the factors influencing a particle's movement within the swarm. On the left, there's a central point labeled \"particle\" with three arrows pointing away from it, each labeled differently: \"inertia,\" \"individual best,\" and \"swarm best.\" This signifies the particle's current position and the three influences on its next move. A pointing hand icon indicates action towards the right side of the image, where a new diagram shows the same central particle now with a single arrow labeled \"new direction.\" This represents the resultant direction after combining the previous three influences. The layout is simple and uses contrasting colors to differentiate the vectors affecting the particle's trajectory.](../../assets/images/pso_1.png \"new direction of a particle\")\n",
    "\n",
    "PSO is particularly notable for its simplicity and the fact that it does not depend on the gradient of the objective function, making it suitable for a wide range of optimization problems. It's also easily parallelizable, allowing for efficient implementation, especially using architectures like map-reduce.\n",
    "\n",
    "One key aspect of PSO is its flexibility in terms of the problem dimensions it can handle. It can be applied to functions with multiple variables, making it adaptable to complex, real-world scenarios.\n",
    "\n",
    "PSO's algorithm involves several iterations where each particle's position is evaluated based on the objective function. If a particle finds a position that is better than its previous best, it updates its personal best. Then, all particles adjust their velocities and move to new positions. This process repeats until a stopping criterion, like a maximum number of iterations or a satisfactory solution, is met.\n",
    "\n",
    "PSO has been compared to genetic algorithms (GAs) in terms of its approach to optimization. While both use iterative processes and random elements, GAs operate more like a Monte Carlo method, randomly generating candidate solutions and selecting the best ones for further competition. In contrast, PSO simulates a flocking behavior, where the movement of individual particles is influenced by their own experience and that of their neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12c7314-7ccd-46de-95a4-0a22010513ae",
   "metadata": {},
   "source": [
    "## Usage examples\n",
    "\n",
    "1. **Engineering Problem Solving**: PSO is employed extensively in engineering to optimize designs and processes. This involves using PSO to find the most efficient solutions in terms of cost, materials, and functionality.\n",
    "\n",
    "2. **Hybrid Methods**: PSO is often combined with deterministic methods like the conjugate gradient method, Newton's method, and quasi-Newton method for improved performance. This hybrid approach takes advantage of both heuristic and deterministic methods, allowing for more accurate and efficient problem-solving.\n",
    "\n",
    "3. **Dynamic Problem Solving**: PSO is effective in dynamic environments where the global optimum changes over time. This adaptability makes it suitable for real-world problems that are dynamic in nature. The algorithm can be enhanced with concepts such as repulsion, dynamic network topologies, or multi-swarms to maintain a diverse solution population and leverage large-scale parallelization, making it highly scalable for modern supercomputing applications.\n",
    "\n",
    "4. **Multi-response Optimization**: PSO is applied in scenarios involving multiple incomparable or conflicting quality features (responses), such as in industrial and scientific processes where several variables must be optimized simultaneously. The algorithm can combine individual responses within a composite objective function, making it an effective tool for complex multi-response optimization tasks.\n",
    "\n",
    "5. **Wave Scattering Problems**: In the field of electromagnetics, PSO is used to solve complex wave scattering problems, such as designing cloaking devices. The algorithm's ability to efficiently explore the solution space and adapt to the specific requirements of these problems makes it a valuable tool for researchers and engineers working in this domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7767bb7",
   "metadata": {},
   "source": [
    "## Strengths\n",
    "\n",
    "1. **Simplicity and Easy Implementation**: PSO is known for its straightforward concept and ease of implementation. It doesn't require gradient information about the problem, making it suitable for a wide range of applications where the objective function's gradient is unknown or difficult to calculate.\n",
    "\n",
    "2. **Efficiency**: It is efficient in finding solutions, often requiring fewer iterations compared to other optimization methods.\n",
    "\n",
    "3. **Flexibility and Adaptability**: PSO can handle optimization problems with multiple variables or higher-dimensional vector space. This flexibility allows it to be applied to complex real-world scenarios where the objective function involves multiple factors.\n",
    "\n",
    "4. **Parallelizability**: PSO can be easily parallelized, as each particle in the swarm can be updated independently. This feature makes it a good fit for modern computing architectures, such as map-reduce, which can significantly speed up the optimization process.\n",
    "\n",
    "5. **Dynamic Adaptation**: Each particle in PSO adapts its traveling velocity and position dynamically, based on its own experience and the experience of other particles in the swarm. This dynamic adaptation helps in efficiently exploring the solution space to find the global optimum.\n",
    "\n",
    "6. **Robustness to Local Optima**: PSO's swarm-based approach helps in avoiding premature convergence to local optima, making it effective in finding global solutions in complex landscapes.\n",
    "\n",
    "7. **Diverse Applications**: PSO has been successfully applied in various fields, including engineering, economics, and data science. Its ability to handle complex optimization problems makes it a versatile tool for researchers and practitioners.\n",
    "\n",
    "5. **No Gradient Requirement**: Unlike some other optimization methods, PSO does not require the gradient of the objective function, making it suitable for problems where the gradient is difficult to compute or does not exist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfb1301",
   "metadata": {},
   "source": [
    "## Weaknesses\n",
    "\n",
    "1. **Risk of Premature Convergence**: PSO can sometimes converge too early to a non-optimal solution, especially in complex problem spaces with many local optima. This can limit its effectiveness in finding the global optimum.\n",
    "\n",
    "2. **Sensitivity to Parameters**: The performance of PSO can be highly sensitive to the choice of its parameters, such as inertia weight and acceleration coefficients. Incorrect parameter settings can lead to poor performance.\n",
    "\n",
    "3. **Difficulty in Handling High-Dimensional Spaces**: PSO can struggle in high-dimensional optimization problems, where it may fall into local optima or experience a slow convergence rate.\n",
    "\n",
    "4. **Neighborhood Topology Influence**: The structure of the neighborhood topology in PSO influences the particle's movement and the extent of social interaction within the swarm. Different topologies, like star, wheel, or ring, can affect the convergence speed and solution quality.\n",
    "\n",
    "5. **Balance Between Exploration and Exploitation**: Achieving the right balance between exploration (searching new areas in the problem space) and exploitation (focusing on promising areas already found) is challenging in PSO. Imbalance can lead to either missing the global optimum or slow convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ede478d",
   "metadata": {},
   "source": [
    "## Python demonstration\n",
    "\n",
    "### PSO on multiple complex functions\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Bellow process is intense. <br> It creates the GIF animations displayed bellow. <br> Refresh the current page to update displayed animations after a new generation\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d535ed",
   "metadata": {},
   "source": [
    "### PSO design, Animation design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2462df00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import imageio.v2 as imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Initialize a random number generator for reproducibility.\n",
    "rng_engine = np.random.default_rng(seed=None)\n",
    "\n",
    "def rastrigin_function(x, y):\n",
    "    \"\"\"\n",
    "    Calculate the Rastrigin function.\n",
    "\n",
    "    The Rastrigin function is a non-convex function used as a performance test\n",
    "    problem for optimization algorithms. It is known for its large number of\n",
    "    local minima.\n",
    "\n",
    "    Args:\n",
    "    - x (float): The x-coordinate.\n",
    "    - y (float): The y-coordinate.\n",
    "\n",
    "    Returns:\n",
    "    - float: The value of the Rastrigin function at (x, y).\n",
    "    \"\"\"\n",
    "    A = 10\n",
    "    n = 2\n",
    "    return A * n + (x**2 - A * np.cos(2 * np.pi * x)) + (y**2 - A * np.cos(2 * np.pi * y))\n",
    "\n",
    "def ackley_function(x, y):\n",
    "    \"\"\"\n",
    "    Calculate the Ackley function.\n",
    "\n",
    "    The Ackley function is widely used for testing optimization algorithms.\n",
    "    It is characterized by a nearly flat outer region and a large hole at the center.\n",
    "\n",
    "    Args:\n",
    "    - x (float): The x-coordinate.\n",
    "    - y (float): The y-coordinate.\n",
    "\n",
    "    Returns:\n",
    "    - float: The value of the Ackley function at (x, y).\n",
    "    \"\"\"\n",
    "    a = 20\n",
    "    b = 0.2\n",
    "    c = 2 * np.pi\n",
    "    sum_sq_term = -0.5 * (x**2 + y**2)\n",
    "    cos_term = np.cos(c * x) + np.cos(c * y)\n",
    "    return -a * np.exp(b * sum_sq_term) - np.exp(0.5 * cos_term) + a + np.exp(1)\n",
    "\n",
    "def rosenbrock_function(x, y, a=1, b=100):\n",
    "    \"\"\"\n",
    "    Calculate the Rosenbrock function.\n",
    "\n",
    "    The Rosenbrock function, also known as the Valley or Banana function,\n",
    "    is a popular test problem for gradient-based optimization algorithms.\n",
    "\n",
    "    Args:\n",
    "    - x (float): The x-coordinate.\n",
    "    - y (float): The y-coordinate.\n",
    "    - a (float, optional): The parameter a. Default is 1.\n",
    "    - b (float, optional): The parameter b. Default is 100.\n",
    "\n",
    "    Returns:\n",
    "    - float: The value of the Rosenbrock function at (x, y).\n",
    "    \"\"\"\n",
    "    return (a - x)**2 + b * (y - x**2)**2\n",
    "\n",
    "def goldstein_price_function(x, y):\n",
    "    \"\"\"\n",
    "    Calculate the Goldstein-Price function.\n",
    "\n",
    "    The Goldstein-Price function is a complex, multimodal function used\n",
    "    as a benchmark in optimization. It has several local minima.\n",
    "\n",
    "    Args:\n",
    "    - x (float): The x-coordinate.\n",
    "    - y (float): The y-coordinate.\n",
    "\n",
    "    Returns:\n",
    "    - float: The value of the Goldstein-Price function at (x, y).\n",
    "    \"\"\"\n",
    "    part1 = (1 + (x + y + 1)**2 * (19 - 14*x + 3*x**2 - 14*y + 6*x*y + 3*y**2))\n",
    "    part2 = (30 + (2*x - 3*y)**2 * (18 - 32*x + 12*x**2 + 48*y - 36*x*y + 27*y**2))\n",
    "    return part1 * part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8877520",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Particle:\n",
    "    def __init__(self, bounds, objective_function):\n",
    "        \"\"\"\n",
    "        Initialize a particle for the PSO algorithm.\n",
    "\n",
    "        Args:\n",
    "        - bounds (numpy.ndarray): A 2D array of shape (n, 2) where 'n' is the number\n",
    "          of dimensions, and the two columns represent the min and max values for each dimension.\n",
    "        - objective_function (callable): The objective function to be optimized.\n",
    "\n",
    "        Attributes:\n",
    "        - position (numpy.ndarray): Current position of the particle in the search space.\n",
    "        - velocity (numpy.ndarray): Current velocity of the particle.\n",
    "        - best_position (numpy.ndarray): The best position encountered by this particle.\n",
    "        - best_value (float): The best objective function value encountered by this particle.\n",
    "        - objective_function (callable): The objective function to be optimized.\n",
    "        \"\"\"\n",
    "        self.position = rng_engine.uniform(bounds[:, 0], bounds[:, 1], size=bounds.shape[0])\n",
    "        self.velocity = rng_engine.uniform(-1, 1, size=bounds.shape[0])\n",
    "        self.best_position = np.copy(self.position)\n",
    "        self.best_value = float('inf')\n",
    "        self.objective_function = objective_function\n",
    "\n",
    "    def update(self, global_best_position):\n",
    "        \"\"\"\n",
    "        Update the particle's velocity and position based on its own experience and\n",
    "        the experience of the swarm.\n",
    "\n",
    "        Args:\n",
    "        - global_best_position (numpy.ndarray): The best position found by the swarm.\n",
    "\n",
    "        The update method uses the standard PSO formula to update the particle's\n",
    "        velocity and position.\n",
    "        \"\"\"\n",
    "        w = 0.5  # inertia weight\n",
    "        c1 = 0.8  # cognitive (particle's best) coefficient\n",
    "        c2 = 0.9  # social (swarm's best) coefficient\n",
    "\n",
    "        r1, r2 = rng_engine.random(2)\n",
    "        self.velocity = (w * self.velocity + \n",
    "                         c1 * r1 * (self.best_position - self.position) + \n",
    "                         c2 * r2 * (global_best_position - self.position))\n",
    "        self.position += self.velocity\n",
    "\n",
    "        value = self.objective_function(*self.position)\n",
    "        if value < self.best_value:\n",
    "            self.best_value = value\n",
    "            self.best_position = np.copy(self.position)\n",
    "\n",
    "class PSO:\n",
    "    def __init__(self, num_particles, bounds, objective_function):\n",
    "        \"\"\"\n",
    "        Initialize the Particle Swarm Optimization algorithm.\n",
    "\n",
    "        Args:\n",
    "        - num_particles (int): The number of particles in the swarm.\n",
    "        - bounds (numpy.ndarray): The bounds of the search space.\n",
    "        - objective_function (callable): The objective function to be optimized.\n",
    "\n",
    "        Attributes:\n",
    "        - particles (list): A list of Particle objects.\n",
    "        - global_best_position (numpy.ndarray): The best position found by the swarm.\n",
    "        - global_best_value (float): The best objective function value found by the swarm.\n",
    "        - trails (list): A list of trails for each particle to visualize their paths.\n",
    "        - objective_function (callable): The objective function to be optimized.\n",
    "        \"\"\"\n",
    "        self.particles = [Particle(bounds, objective_function) for _ in range(num_particles)]\n",
    "        self.global_best_position = np.zeros(bounds.shape[0])\n",
    "        self.global_best_value = float('inf')\n",
    "        self.trails = [[] for _ in range(num_particles)]  # Store trails for each particle\n",
    "        self.objective_function = objective_function\n",
    "\n",
    "    def optimize(self, num_iterations):\n",
    "        \"\"\"\n",
    "        Perform optimization over a set number of iterations.\n",
    "\n",
    "        Args:\n",
    "        - num_iterations (int): The number of iterations to perform.\n",
    "\n",
    "        This method iterates over the specified number of iterations, updating\n",
    "        each particle and potentially the global best position and value.\n",
    "        \"\"\"\n",
    "        for t in range(num_iterations):\n",
    "            for index, particle in enumerate(self.particles):\n",
    "                particle.update(self.global_best_position)\n",
    "                self.trails[index].append(particle.position.copy())\n",
    "\n",
    "                if particle.best_value < self.global_best_value:\n",
    "                    self.global_best_value = particle.best_value\n",
    "                    self.global_best_position = particle.best_position.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af817eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_frame(frame, trails, function, function_name, X, Y, Z, known_optima, save_path):\n",
    "    \"\"\"\n",
    "    Generate and save a plot for a specific frame in the optimization process.\n",
    "\n",
    "    This function creates a 2D and 3D visualization of the particles' positions at a given iteration.\n",
    "\n",
    "    Args:\n",
    "    - frame (int): The current iteration number.\n",
    "    - trails (list): Trails of the particles, showing their positions across iterations.\n",
    "    - function (callable): The objective function being optimized.\n",
    "    - function_name (str): The name of the objective function.\n",
    "    - X, Y, Z (numpy.ndarray): Meshgrid arrays for plotting.\n",
    "    - known_optima (list): Coordinates of known optima for the function, for visualization.\n",
    "    - save_path (str): Path to save the generated plot.\n",
    "\n",
    "    The function generates a 2D contour plot and a 3D surface plot to show the function's landscape and the particles' positions.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "    fig.suptitle(f\"{function_name} function - Iteration {frame}\", fontsize=16)\n",
    "\n",
    "    # 2D plot\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax1.contourf(X, Y, Z, levels=200, cmap='viridis')\n",
    "    for trail in trails:\n",
    "        if len(trail) > frame:\n",
    "            ax1.plot(*trail[frame], 'ro')\n",
    "    ax1.set_title(\"2D View\")\n",
    "\n",
    "    # 3D plot\n",
    "    ax2 = fig.add_subplot(122, projection='3d')\n",
    "    ax2.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap='viridis', edgecolor='none', alpha=0.7)\n",
    "    for trail in trails:\n",
    "        if len(trail) > frame:\n",
    "            ax2.scatter(*trail[frame], function(*trail[frame]), color='r', s=50)\n",
    "\n",
    "    for opt in known_optima:\n",
    "        ax1.scatter(opt[0], opt[1], color='green', s=300, marker='x', label='Known Optima')\n",
    "        ax2.scatter(opt[0], opt[1], opt[2], color='green', s=500, marker='x', label='Known Optima')\n",
    "\n",
    "    ax2.set_title(\"3D View\")\n",
    "    ax2.set_xlabel('X axis')\n",
    "    ax2.set_ylabel('Y axis')\n",
    "    ax2.set_zlabel('Z axis')\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.close(fig)\n",
    "    fig.savefig(save_path)\n",
    "\n",
    "def create_gif(function, function_name, num_iterations, bounds, num_particles, known_optima):\n",
    "    \"\"\"\n",
    "    Create a GIF animation to visualize the optimization process of PSO.\n",
    "\n",
    "    This function runs the PSO algorithm and generates a series of plots at each iteration,\n",
    "    which are then compiled into a GIF.\n",
    "\n",
    "    Args:\n",
    "    - function (callable): The objective function being optimized.\n",
    "    - function_name (str): The name of the objective function.\n",
    "    - num_iterations (int): The number of iterations for the PSO algorithm.\n",
    "    - bounds (numpy.ndarray): The bounds of the search space.\n",
    "    - num_particles (int): The number of particles in the swarm.\n",
    "    - known_optima (list): Coordinates of known optima for visualization.\n",
    "\n",
    "    The function saves the generated GIF in a specified directory.\n",
    "    \"\"\"\n",
    "    pso = PSO(num_particles, bounds, function)\n",
    "    pso.optimize(num_iterations)\n",
    "\n",
    "    # Prepare for visualization\n",
    "    x = np.linspace(bounds[0, 0], bounds[0, 1], 200)\n",
    "    y = np.linspace(bounds[1, 0], bounds[1, 1], 200)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = function(X, Y)\n",
    "\n",
    "    gif_dir = f\"./assets/images/{function_name}\"\n",
    "    os.makedirs(gif_dir, exist_ok=True)\n",
    "    filenames = []\n",
    "    for frame in range(num_iterations):\n",
    "        frame_path = f\"{gif_dir}/frame_{frame:04d}.png\"\n",
    "        plot_frame(frame, pso.trails, function, function_name, X, Y, Z, known_optima, frame_path)\n",
    "        filenames.append(frame_path)\n",
    "    \n",
    "    # Create the GIF\n",
    "    gif_path = f\"{gif_dir}/{function_name}.gif\"\n",
    "    with imageio.get_writer(gif_path, mode='I', duration=0.2, loop=0) as writer:\n",
    "        for filename in filenames:\n",
    "            image = imageio.imread(filename)\n",
    "            writer.append_data(image)\n",
    "\n",
    "    # Clean up individual frames\n",
    "    for filename in filenames:\n",
    "        os.remove(filename)\n",
    "\n",
    "    best_position = pso.global_best_position\n",
    "    best_value = pso.global_best_value\n",
    "    print(f\"Best solution for {function_name} (x, y) = ({best_position[0]}, {best_position[1]}) with value z = {best_value} \\n Saved at {gif_path} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca62fcb",
   "metadata": {},
   "source": [
    "### Hyperparameters setting & execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2fb031f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best solution for Rastrigin (x, y) = (-1.356925599573995e-09, 8.917626660388975e-10) with value z = 0.0 \n",
      " Saved at ./assets/images/Rastrigin/Rastrigin.gif \n",
      "\n"
     ]
    }
   ],
   "source": [
    "bounds = np.array([[-5, 5], [-5, 5]]) \n",
    "# bounds:\n",
    "# This parameter defines the search space for each dimension in the PSO algorithm. In this case, the bounds are a \n",
    "# 2D array where each row specifies the lower and upper limits for a dimension. For instance, `np.array([[-5, 5], \n",
    "# [-5, 5]])` sets the search space to be between -5 and 5 for both dimensions. Properly setting the bounds is \n",
    "# essential to ensure effective exploration and that the particles do not wander off into irrelevant areas of the search \n",
    "# space. Appropriate bounds help in striking a balance between efficient exploration and the likelihood of finding the \n",
    "# global optimum.\n",
    "\n",
    "num_particles = 100\n",
    "# num_particles:\n",
    "# This defines the size of the swarm, i.e., the number of particles in the swarm. Each particle represents a potential \n",
    "# solution in the multidimensional search space. A larger swarm size typically allows for better exploration of the search \n",
    "# space, increasing the probability of finding a global optimum. However, more particles also mean a higher computational \n",
    "# cost, so there's a trade-off between the thoroughness of the search and the resources required. The optimal number \n",
    "# of particles may vary depending on the complexity of the problem and the computational power available.\n",
    "\n",
    "num_iterations = 70\n",
    "# num_iterations:\n",
    "# This sets the number of iterations (or time steps) for which the PSO algorithm will run. Each iteration allows the \n",
    "# particles to move and adjust their positions in the search space, guided by their own experience and that of their \n",
    "# neighbors. A higher number of iterations provides more opportunities for the swarm to converge towards the optimum. \n",
    "# However, this also means increased computational time. Balancing the number of iterations is crucial; too few may \n",
    "# result in suboptimal solutions, while too many can lead to unnecessary computational overhead. The choice of iteration \n",
    "# count should be based on the desired level of solution accuracy and the complexity of the optimization problem.\n",
    "\n",
    "\n",
    "# Known optima for display purpose\n",
    "known_optima_rastrigin = [(0, 0, 0)]\n",
    "known_optima_ackley = [(0, 0, 0)]\n",
    "known_optima_rosenbrock = [(1, 1, 0)]\n",
    "known_optima_goldstein_price = [(0, -1, 3)]\n",
    "\n",
    "# Create GIFs for each function (execution) # Comment to disable\n",
    "create_gif(rastrigin_function, 'Rastrigin', num_iterations, bounds, num_particles, known_optima_rastrigin)\n",
    "create_gif(ackley_function, 'Ackley', num_iterations, bounds, num_particles, known_optima_ackley)\n",
    "create_gif(rosenbrock_function, 'Rosenbrock', num_iterations, bounds, num_particles, known_optima_rosenbrock)\n",
    "create_gif(goldstein_price_function, 'Goldstein_Price', num_iterations, bounds, num_particles, known_optima_goldstein_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13db02c9",
   "metadata": {},
   "source": [
    "![](../../assets/images/ackley/ackley.gif)\n",
    "<br>\n",
    "\n",
    "![](../../assets/images/Goldstein_Price/Goldstein_Price.gif)\n",
    "<br>\n",
    "\n",
    "![](../../assets/images/rastrigin/rastrigin.gif)\n",
    "<br>\n",
    "\n",
    "![](../../assets/images/rosenbrock/rosenbrock.gif)\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of demonstration\n",
    "\n",
    "---\n",
    "\n",
    "## Practical optimization tools\n",
    "\n",
    "1. [**DEAP (Python):** ](https://github.com/DEAP/deap) Distributed Evolutionary Algorithms in Python, or DEAP, is an open-source library specifically designed for evolutionary computation. It provides tools for the implementation of various evolutionary algorithms, including Particle Swarm Optimization (PSO). DEAP is known for its flexibility and ease of use, allowing users to easily customize evolutionary algorithms for their specific problems.\n",
    "\n",
    "2. [**jMetalPy (Python):** ](https://jmetal.github.io/jMetalPy/index.html) jMetalPy is an open-source Python framework for multi-objective optimization with metaheuristic algorithms. It includes a variety of algorithms, among which is the Particle Swarm Optimization. This framework is popular for its focus on multi-objective problems and for providing a rich set of features to analyze and visualize the results of the optimization process.\n",
    "\n",
    "3. [**Apache Mahout (Java):** ](https://mahout.apache.org/) Apache Mahout is a scalable machine learning and data mining library, primarily for Java. While it's more known for its machine learning capabilities, it also offers various optimization algorithms, including Particle Swarm Optimization. It's popular in the Java community for its scalability and integration with Hadoop ecosystems.\n",
    "\n",
    "4. [**Opt4J (Java):** ](https://sdarg.github.io/opt4j/) Opt4J is an open-source framework for evolutionary computation, multi-objective optimization, and heuristic search. Written in Java, it includes a modular structure that makes it highly extensible. Particle Swarm Optimization is one of the many optimization algorithms provided by Opt4J. It's widely recognized for its user-friendly graphical interface and its ability to combine different optimization techniques.\n",
    "\n",
    "5. [**PaGMO/PyGMO (C++ and Python):**](https://www.esa.int/gsp/ACT/open_source/pagmo/) PaGMO (and its Python counterpart PyGMO) is an open-source scientific library for massively parallel optimization. Originally written in C++, it provides an extensive collection of algorithms for optimization, including Particle Swarm Optimization. It is particularly popular for its ability to handle large-scale optimization problems and for its support of parallel and distributed computing, making it a robust choice for complex tasks. The library also offers a Python interface, broadening its accessibility and ease of use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d457aaf3-dad6-4935-8c0a-e0e59091d3cc",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "| Sources |\n",
    "|---------|\n",
    "| [Particle swarm optimization - Wikipedia](https://en.wikipedia.org/wiki/Particle_swarm_optimization) |\n",
    "| [Optimisation par Essaim Particulaire (French) - Youtube](https://www.youtube.com/watch?v=yVcqUqJG7Ic) |\n",
    "| [3D Convergence animation of PSO - Youtube](https://www.youtube.com/watch?v=UmAzMupAUZ8) |\n",
    "| [Particle Swarm Optimization Algorithms with Applications to Wave Scattering Problems - Intechopen](https://www.intechopen.com/chapters/76395) |\n",
    "| [Particle Swarm Optimization: A Powerful Technique for Solving Engineering Problems - Intechopen](https://www.intechopen.com/chapters/69586) |\n",
    "| [An introduction to PSO - Analyticsvidhya](https://www.analyticsvidhya.com/blog/2021/10/an-introduction-to-particle-swarm-optimization-algorithm/) |\n",
    "| [A Gentle Introduction to Particle Swarm Optimization - Machinelearningmastery](https://machinelearningmastery.com/a-gentle-introduction-to-particle-swarm-optimization/) |\n",
    "| [A Comprehensive Survey on Particle Swarm Optimization Algorithm and Its Applications - Hindawi](https://www.hindawi.com/journals/mpe/2015/931256/) |\n",
    "| [An Adaptive Particle Swarm Optimization Algorithm Based on Directed Weighted Complex Network - Hindawi](https://www.hindawi.com/journals/mpe/2014/434972/) |\n",
    "| [Dynamic particle swarm optimization of biomolecular simulation parameters with flexible objective functions - Nature.com](https://www.nature.com/articles/s42256-021-00366-3) |\n",
    "| [Fishes picture](https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.forum-photosub.fr%2Fforum%2Fviewtopic.php%3Ft%3D19052&psig=AOvVaw0ToRfoyJjFcQ-9OCqTqZj8&ust=1702501953327000&source=images&cd=vfe&opi=89978449&ved=0CBEQjhxqFwoTCLiG3JfoioMDFQAAAAAdAAAAABAD) |\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
