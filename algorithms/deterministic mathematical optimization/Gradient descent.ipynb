{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba90010e",
   "metadata": {},
   "source": [
    "# Gradient descent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f14dc7",
   "metadata": {},
   "source": [
    "## Understanding the algorithm\n",
    "\n",
    "/!\\ Rajouter que: Méthode fondamentale d'optimisation continue pour minimiser les fonctions différentiables.\n",
    "\n",
    "Gradient descent is an optimization algorithm used to minimize a function iteratively. Given a function $ f(x) $, the algorithm starts with an initial guess $ x_0 $ for the minimum and iteratively refines this guess. In each iteration, the gradient $ \\nabla f(x) $ of the function at the current guess is computed. The gradient is a vector that points in the direction of the steepest ascent of the function. To minimize the function, one moves in the direction opposite to the gradient, updating the current guess according to the formula:\n",
    "\n",
    "$$\n",
    "x_{\\text{new}} = x_{\\text{old}} - \\alpha \\nabla f(x_{\\text{old}})\n",
    "$$\n",
    "\n",
    "Here, $ \\alpha $ is the learning rate, a hyperparameter that controls the step size. This process is repeated until the function value $ f(x) $ converges to a minimum value, within some predefined tolerance level.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6467fdd0",
   "metadata": {},
   "source": [
    "## Usage examples\n",
    "\n",
    "- **Machine Learning and Deep Learning**: Used extensively for optimizing loss functions in various machine learning algorithms (neural networks).\n",
    "   \n",
    "- **Optimization Problems**: In operations research, for solving linear and non-linear optimization problems.\n",
    "  \n",
    "- **Natural Language Processing**: In algorithms for text analysis and sentiment classification.\n",
    "  \n",
    "- **Computer Vision**: For tasks like image recognition and object detection.\n",
    "\n",
    "- **Control Systems**: For optimizing control functions.\n",
    "\n",
    "- **Finance**: In portfolio optimization and algorithmic trading strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a35d0df",
   "metadata": {},
   "source": [
    "## Strengths\n",
    "\n",
    "1. **Simplicity**: The algorithm is simple to understand and easy to implement.\n",
    "\n",
    "2. **Efficiency**: For large-scale problems, the algorithm can be more efficient than other optimization methods like the simplex method.\n",
    "\n",
    "3. **Flexibility**: Can be adapted for various types of objective functions, including those that are non-differentiable using subgradients.\n",
    "\n",
    "4. **Scalability**: Variants like stochastic gradient descent can handle very large datasets and high dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3624dd-6e6d-4b30-9b2c-5b33c3d02ce8",
   "metadata": {},
   "source": [
    "## Weaknesses\n",
    "\n",
    "1. **Local Minima**: For non-convex functions, gradient descent can get stuck in local minima and fail to find the global minimum.\n",
    "\n",
    "2. **Sensitivity to Learning Rate**: The learning rate $ \\alpha $ needs to be carefully chosen. A rate that's too large can overshoot the minimum, while a rate that's too small can make the algorithm slow to converge or get stuck.\n",
    "\n",
    "3. **Computational Cost**: Calculating the gradient can be computationally expensive for complex functions.\n",
    "\n",
    "4. **Numerical Errors**: For very flat or very steep regions, numerical errors can become significant.\n",
    "\n",
    "5. **Initialization Dependent**: The initial point $ x_0 $ can have a significant impact on the convergence of the algorithm, especially for non-convex functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5ffb1c",
   "metadata": {},
   "source": [
    "#### Stopping conditions\n",
    "- maxit, i.e. a predetermined maximum number of iterations. \n",
    "- abstol, i.e., stop when the function gets \"close enough\" to zero. \n",
    "- reltol, which is like your second suggestion, stop when the improvement drops below a threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270e6b74",
   "metadata": {},
   "source": [
    "## Python demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9bc0306a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numdifftools as nd\n",
    "import numpy as np\n",
    "from autograd import grad\n",
    "import numpy as np\n",
    "\n",
    "# Define objective function\n",
    "objective_function = lambda x: (x[0] ** 2) + (3 * (x[1] ** 2))\n",
    "\n",
    "# Initial point\n",
    "P0 = np.array([2.0, 1.0])\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.2\n",
    "\n",
    "# Stopping tolerance\n",
    "tol = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "156c2b43-c6ac-46e6-b080-22aaa2b89062",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient_descent(function, initial_point, learning_rate, tolerance):\n",
    "    point = initial_point\n",
    "    nb_iter = 0\n",
    "    old_loss = function(point)\n",
    "    gradient_function = grad(function)\n",
    "    \n",
    "    while True:\n",
    "        nb_iter += 1\n",
    "        gradient = np.array(gradient_function(point))\n",
    "        \n",
    "        # Update rule\n",
    "        point = point - (learning_rate * gradient)\n",
    "        \n",
    "        # Compute new loss\n",
    "        new_loss = function(point)\n",
    "\n",
    "        # Check for convergence\n",
    "        if abs(new_loss - old_loss) < tolerance:\n",
    "            print(\"Number of iterations:\", nb_iter)\n",
    "            break\n",
    "            \n",
    "        old_loss = new_loss\n",
    "\n",
    "    return point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "147935cf-fc59-483d-9443-3b8a0053d184",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 16\n",
      "The minimum occurs at: [5.64221981e-04 6.55360000e-12]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.183464443978558e-07"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run of the algorithm\n",
    "res = gradient_descent(objective_function, P0, lr, tol)\n",
    "print(\"The minimum occurs at:\", res)\n",
    "objective_function(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9388075f-cd99-4136-9edd-849673df2247",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 16\n",
      "3.183464443978558e-07\n",
      "Test 1 Passed!\n",
      "Number of iterations: 17\n",
      "4.0111651994129803e-07\n",
      "Test 2 Passed!\n",
      "Number of iterations: 17\n",
      "2.8651179995799273e-07\n",
      "Test 3 Passed!\n"
     ]
    }
   ],
   "source": [
    "def is_close(a, b, tol=1e-6):\n",
    "    print(np.abs(a - b))\n",
    "    return np.all(np.abs(a - b) < tol)\n",
    "\n",
    "# Test 1: Objective function f(x, y) = x^2 + 3y^2\n",
    "objective_function_2d = lambda x: (x[0] ** 2) + (3 * (x[1] ** 2))\n",
    "P0_2d = np.array([2.0, 1.0])\n",
    "lr = 0.2\n",
    "tol = 1e-6\n",
    "res_2d = gradient_descent(objective_function_2d, P0_2d, lr, tol)\n",
    "if is_close(objective_function_2d(res_2d), 0.0):\n",
    "    print(\"Test 1 Passed!\")\n",
    "else:\n",
    "    print(\"Test 1 Failed!\")\n",
    "    \n",
    "# Test 2: Objective function f(x, y, z) = x^2 + y^2 + z^2\n",
    "objective_function_3d = lambda x: x[0] ** 2 + x[1] ** 2 + x[2] ** 2\n",
    "P0_3d = np.array([2.0, 1.0, 3.0])\n",
    "res_3d = gradient_descent(objective_function_3d, P0_3d, lr, tol)\n",
    "if is_close(objective_function_3d(res_3d), 0.0):\n",
    "    print(\"Test 2 Passed!\")\n",
    "else:\n",
    "    print(\"Test 2 Failed!\")\n",
    "    \n",
    "# Test 3: Objective function f(x, y) = (x - 1)^2 + (y + 2)^2\n",
    "objective_function_shifted_2d = lambda x: (x[0] - 1) ** 2 + (x[1] + 2) ** 2\n",
    "P0_shifted_2d = np.array([2.0, 1.0])\n",
    "res_shifted_2d = gradient_descent(objective_function_shifted_2d, P0_shifted_2d, lr, tol)\n",
    "if is_close(objective_function_shifted_2d(res_shifted_2d), 0.0):\n",
    "    print(\"Test 3 Passed!\")\n",
    "else:\n",
    "    print(\"Test 3 Failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of demonstration\n",
    "\n",
    "---\n",
    "\n",
    "## Practical optimization tools\n",
    "\n",
    "1. [**TensorFlow (Python):**](https://www.tensorflow.org) TensorFlow, an open-source library developed by Google, is renowned for its flexible architecture that enables easy deployment of computation across various platforms. It's particularly strong in training and inference of deep neural networks and includes sophisticated optimization algorithms like gradient descent and its variants. TensorFlow is favored for its scalability, extensive community support, and comprehensive suite of tools for machine learning and deep learning.\n",
    "\n",
    "2. [**Deeplearning4j (Java):**](https://deeplearning4j.konduit.ai/) Deeplearning4j is a prominent open-source, distributed deep-learning library for Java and Scala. It integrates with Hadoop and Apache Spark and is designed to be used in business environments on distributed GPUs and CPUs. Deeplearning4j includes implementations of various optimization algorithms, including different forms of gradient descent. It's particularly valued for its native support for Java, which is a widely used language in enterprise environments, allowing the seamless integration of deep learning into existing Java-based infrastructure.\n",
    "\n",
    "3. [**PyTorch (Python):**](https://pytorch.org/) PyTorch, another popular open-source machine learning library, is known for its ease of use, flexibility, and dynamic computational graph. It provides a variety of optimization algorithms including gradient descent variants, which are integral for training neural networks. PyTorch is widely adopted in the research community for its intuitive design and efficient performance in both development and research settings.\n",
    "\n",
    "4. [**Keras (Python):**](https://keras.io/) Keras is a high-level neural networks API, capable of running on top of TensorFlow, Theano, or Microsoft Cognitive Toolkit. It simplifies the implementation of deep learning models, including the use of various gradient descent algorithms. Keras is praised for its user-friendliness, modularity, and extensibility, making it suitable for both beginners and experienced practitioners in deep learning.\n",
    "\n",
    "5. [**Apache MXNet (Python, R, Scala, C++, Julia):**](https://mxnet.apache.org/) Apache MXNet is a multi-language machine learning library known for its efficiency in large-scale deep learning models. It supports a range of optimization algorithms, including gradient descent and its variants. MXNet stands out for its scalability across multiple GPUs and distributed computing environments, and its support for a variety of programming languages broadens its appeal to a wide array of developers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
