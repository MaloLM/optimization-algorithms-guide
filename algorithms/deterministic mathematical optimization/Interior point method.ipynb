{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6d5ef08",
   "metadata": {},
   "source": [
    "# Interior point method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abfa78c",
   "metadata": {},
   "source": [
    "## Understanding the algorithm\n",
    "\n",
    "The core idea of the Interior Point Algorithm involves transforming a constrained optimization problem into an unconstrained one. This is achieved by incorporating a barrier function into the objective function of the optimization problem. The barrier function is a smooth convex function, defined within the interior of the feasible region of the problem, which approaches infinity as the solution approaches the boundary of this region. By doing this, the Interior Point Algorithm effectively guides the solution process to remain within the feasible region.\n",
    "\n",
    "### Applications and Suitability\n",
    "The Interior Point Algorithm is well-suited for large-scale linear programming problems due to its polynomial-time complexity. It has also been effectively adapted to non-linear programming, making it a versatile tool in optimization. Its applications span various fields, including but not limited to financial modeling, resource allocation, and logistics.\n",
    "\n",
    "### Algorithm's Steps and Key Concepts\n",
    "1. **Path-Following Methods**: This approach involves tracking a continuous function, defined by a series of penalty parameters, through an increasing sequence. The function is optimized at each step using methods like Newton's method. The challenge here is maintaining polynomial time efficiency as the solution approaches the region's boundary.\n",
    "\n",
    "2. **Barrier Function**: Integral to path-following methods, the barrier function, often logarithmic in nature, is combined with the objective function. It ensures that as the solution nears the boundary, the function becomes steep, deterring boundary solutions.\n",
    "\n",
    "3. **Primal-Dual Methods**: These methods are different from barrier methods in that they iterate through primal and dual feasible solutions, converging to the optimal solution from both sides. They are generally more efficient as they do not necessarily require separate inner and outer loops.\n",
    "\n",
    "4. **Convergence and Efficiency**: The algorithm is designed to converge to the optimal solution by progressively refining the approximation. The number of steps required for convergence depends on the complexity of the problem and the method's specific implementation, like the choice of the barrier function and update rules for the penalty parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12c7314-7ccd-46de-95a4-0a22010513ae",
   "metadata": {},
   "source": [
    "## Usage examples\n",
    "\n",
    "1. **Finance**: The interior point method is used in finance for various applications, including portfolio management and optimization. This method helps in solving convex optimization problems, which are common in financial modeling and decision-making. It aids in finding optimal solutions for investment strategies, asset allocation, and risk management, by handling complex, multi-variable scenarios efficiently. This approach is particularly valuable due to its ability to deal with large-scale optimization problems typical in financial applications.\n",
    "\n",
    "2. **Signal Processing**: Interior point methods are crucial in the field of signal processing. They are used to solve optimization problems that arise in this area, such as filtering, signal reconstruction, and data compression. These methods are particularly useful for handling large datasets and complex signal structures, where traditional methods might be inefficient.\n",
    "\n",
    "3. **Machine Learning**: In machine learning, interior point methods are applied to solve optimization problems, especially in training models like support vector machines and deep neural networks. They help in efficiently finding the optimal parameters of these models, thus enhancing their predictive accuracy and performance.\n",
    "\n",
    "4. **Control and Mechanical Engineering**: These methods are widely used in control and mechanical engineering for system design and analysis. They help in optimizing various parameters of control systems, such as stability and response time, and in mechanical engineering for designing systems with optimal structural integrity and efficiency.\n",
    "\n",
    "5. **Digital and Analog Circuit Design**: In the field of electronics, particularly in the design of digital and analog circuits, interior point methods are employed to optimize circuit parameters. This ensures optimal performance, energy efficiency, and minimizes the size and cost of the circuits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7767bb7",
   "metadata": {},
   "source": [
    "## Strengths\n",
    "\n",
    "1. **Polynomial Runtime**: Interior point methods have a polynomial runtime for linear and non-linear convex optimization problems, contrasting with the simplex method, which can have exponential runtime in the worst case. This efficiency is achieved through self-concordant barrier functions and the path-following method, making these methods particularly suitable for solving various classes of convex programs efficiently.\n",
    "\n",
    "2. **Handling Non-Linear Programs**: Yuri Nesterov extended the interior point method to non-linear programs. The method is effective for these programs due to the main property of the logarithmic barrier being self-concordant with a finite barrier parameter, allowing for solving many classes of convex programs in polynomial time.\n",
    "\n",
    "3. **Efficient Convergence**: The convergence rate of interior point methods is structured, with the number of Newton steps required to transition from one approximation to the next being fixed and dependent only on specific parameters. This structured approach ensures efficient progression towards the solution.\n",
    "\n",
    "4. **Accuracy and Complexity Management**: The solution accuracy of interior point methods is proportional to the inverse of the penalty parameter, and to achieve higher accuracy, it is sufficient to multiply the penalty parameter by a constant factor. This method controls the total complexity effectively, making it manageable for large-scale problems.\n",
    "\n",
    "5. **Flexibility with Constraints and Objectives**: Interior point methods are versatile in handling different types of constraints and objectives. They work well with linear functions and can be adapted to various forms of convex optimization problems, making them highly adaptable to diverse data science applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfb1301",
   "metadata": {},
   "source": [
    "## Weaknesses\n",
    "\n",
    "1. **Increasing Complexity with Steeper Functions**: As the penalty parameter grows, the solution approaches the boundary, making the function steeper and increasing the runtime of solvers like Newton's method. This complexity makes it challenging to ensure that the total runtime remains polynomial.\n",
    "\n",
    "2. **Dependency on Self-Concordant Barriers**: The method relies heavily on self-concordant barriers for the feasible region, which can be a limiting factor in certain types of optimization problems.\n",
    "\n",
    "3. **Initial Solution Dependency**: The effectiveness of the method can be sensitive to the initial solution. Choosing a poor starting point can affect the convergence rate and overall efficiency.\n",
    "\n",
    "4. **Complex Barrier Function Requirements**: The method requires efficiently computing the value, gradient, and Hessian of the barrier function for every point in the interior of the feasible region, which can be computationally intensive.\n",
    "\n",
    "5. **Fixed Number of Newton Steps**: The method dictates a fixed number of Newton steps to transition from one approximation to the next, which might not be optimal for all problem types and can potentially limit its efficiency in certain cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ede478d",
   "metadata": {},
   "source": [
    "## Python demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c418c639-2bfa-462a-ad4c-f3c426a8ac0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point optimal trouvé: ( 4.883844647167765e-07 , 1.1823292323622598e-07 )\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Fonction objectif\n",
    "def objective_function(x, y, t, a, b, c, d, e, f):\n",
    "    return x**2 + y**2 - t*(math.log(x) + math.log(y) + \n",
    "                            math.log(c - a*x - b*y) + \n",
    "                            math.log(f - d*x - e*y))\n",
    "\n",
    "# Gradient de la fonction objectif\n",
    "def gradient(x, y, t, a, b, c, d, e, f):\n",
    "    grad_x = 2*x - t*(-a/(c - a*x - b*y) - d/(f - d*x - e*y) + 1/x)\n",
    "    grad_y = 2*y - t*(-b/(c - a*x - b*y) - e/(f - d*x - e*y) + 1/y)\n",
    "    return grad_x, grad_y\n",
    "\n",
    "# Méthode de descente de gradient avec enregistrement du chemin\n",
    "def gradient_descent(x0, y0, t, a, b, c, d, e, f, learning_rate, max_iterations):\n",
    "    x, y = x0, y0\n",
    "    path = [(x, y)]  # Enregistrer le chemin\n",
    "    for _ in range(max_iterations):\n",
    "        grad_x, grad_y = gradient(x, y, t, a, b, c, d, e, f)\n",
    "        x -= learning_rate * grad_x\n",
    "        y -= learning_rate * grad_y\n",
    "        path.append((x, y))\n",
    "\n",
    "        # Mise à jour de t pour réduire l'impact de la barrière\n",
    "        t *= 0.9\n",
    "\n",
    "        # Vérifier les conditions de convergence (optionnel)\n",
    "        if math.fabs(grad_x) < 1e-6 and math.fabs(grad_y) < 1e-6:\n",
    "            break\n",
    "\n",
    "    return x, y, path\n",
    "\n",
    "# Paramètres initiaux\n",
    "x0, y0 = 3, 0.5\n",
    "t = 1\n",
    "a, b, c, d, e, f = 1, 1, 10, 1, 1, 10\n",
    "learning_rate = 0.01\n",
    "max_iterations = 1000\n",
    "\n",
    "# Exécution de la méthode de descente de gradient\n",
    "optimal_x, optimal_y, central_path = gradient_descent(x0, y0, t, a, b, c, d, e, f, learning_rate, max_iterations)\n",
    "print(\"Point optimal trouvé: (\", optimal_x, \",\", optimal_y, \")\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d457aaf3-dad6-4935-8c0a-e0e59091d3cc",
   "metadata": {},
   "source": [
    "End of demonstration\n",
    "\n",
    "---\n",
    "## Practical optimization tools\n",
    "\n",
    "For open-source solutions utilizing the interior point method, consider the following libraries:\n",
    "\n",
    "1. [**SciPy (Python):**](https://scipy.org)  A comprehensive library for scientific computing in Python, SciPy includes the `scipy.optimize` module. This module offers a range of optimization algorithms, with capabilities for linear programming using interior point methods. It's widely used due to its integration with the Python ecosystem and its balance between ease of use and powerful optimization capabilities.\n",
    "\n",
    "2. [**CVXOPT (Python):**](https://cvxopt.org) Dedicated to convex optimization, CVXOPT provides specialized functions for solving linear and quadratic programming problems using interior point methods. It is particularly useful for problems in this domain due to its focus on convex problems, offering robust and efficient algorithm implementations.\n",
    "\n",
    "3. [**COIN-OR (C++/Various):**](https://www.coin-or.org) An umbrella for open-source software for the operations research community, COIN-OR houses several libraries like Ipopt (Interior Point OPTimizer). Ipopt is designed for large-scale nonlinear optimization, leveraging interior point methods. This makes it ideal for complex optimization problems in both academic research and industrial applications.\n",
    "\n",
    "4. [**GLPK (GNU Linear Programming Kit):**](https://www.gnu.org/software/glpk/) GLPK is designed for solving large-scale linear programming (LP), mixed-integer programming (MIP), and other related problems. It includes an interior point solver for LP, making it a good choice for projects that require solving these specific types of optimization problems, especially in the context of open-source and GNU-based environments.\n",
    "\n",
    "5. [**Eigen (C++):**](https://eigen.tuxfamily.org/index.php?title=Main_Page) Primarily a linear algebra library, Eigen is known for its versatility and high performance in matrix operations. While it doesn't directly implement interior point methods, it's often used alongside other libraries to develop such solutions, especially in applications that require intensive linear algebra computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "| Sources |\n",
    "|---------|\n",
    "| [Interior-point method - Wikipedia](https://en.wikipedia.org/wiki/Interior-point_method) |\n",
    "| [Interior-point method for NLP - Cornell University](http://optimization.cbe.cornell.edu/index.php?title=Interior-point_method_for_NLP) |\n",
    "| [Interior-point method for LP - Cornell University](https://optimization.cbe.cornell.edu/index.php?title=Interior-point_method_for_LP) |\n",
    "| [Krylov Solvers for Interior Point Methods with Applications in Radiation Therapy](https://ar5iv.labs.arxiv.org/html/2308.00637) |\n",
    "| [Convex Optimization I - Stanford](https://web.stanford.edu/class/ee364a/) |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
