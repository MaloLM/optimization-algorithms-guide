{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba90010e",
   "metadata": {},
   "source": [
    "# Gradient descent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f14dc7",
   "metadata": {},
   "source": [
    "## What is Gradient Descent?\n",
    "\n",
    "Gradient descent is an optimization algorithm used to minimize a function iteratively. Given a function $ f(x) $, the algorithm starts with an initial guess $ x_0 $ for the minimum and iteratively refines this guess. In each iteration, the gradient $ \\nabla f(x) $ of the function at the current guess is computed. The gradient is a vector that points in the direction of the steepest ascent of the function. To minimize the function, one moves in the direction opposite to the gradient, updating the current guess according to the formula:\n",
    "\n",
    "$$\n",
    "x_{\\text{new}} = x_{\\text{old}} - \\alpha \\nabla f(x_{\\text{old}})\n",
    "$$\n",
    "\n",
    "Here, $ \\alpha $ is the learning rate, a hyperparameter that controls the step size. This process is repeated until the function value $ f(x) $ converges to a minimum value, within some predefined tolerance level.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6467fdd0",
   "metadata": {},
   "source": [
    "\n",
    "## Uses of Gradient Descent\n",
    "\n",
    "1. **Machine Learning and Deep Learning**: Used extensively for optimizing loss functions in various machine learning algorithms, especially in neural networks.\n",
    "   \n",
    "2. **Optimization Problems**: In operations research, for solving linear and non-linear optimization problems.\n",
    "  \n",
    "3. **Natural Language Processing**: In algorithms for text analysis and sentiment classification.\n",
    "  \n",
    "4. **Computer Vision**: For tasks like image recognition and object detection.\n",
    "\n",
    "5. **Control Systems**: For optimizing control functions.\n",
    "\n",
    "6. **Finance**: In portfolio optimization and algorithmic trading strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a35d0df",
   "metadata": {},
   "source": [
    "## Advantages of Gradient Descent\n",
    "\n",
    "1. **Simplicity**: The algorithm is simple to understand and easy to implement.\n",
    "\n",
    "2. **Efficiency**: For large-scale problems, the algorithm can be more efficient than other optimization methods like the simplex method.\n",
    "\n",
    "3. **Flexibility**: Can be adapted for various types of objective functions, including those that are non-differentiable using subgradients.\n",
    "\n",
    "4. **Scalability**: Variants like stochastic gradient descent can handle very large datasets and high dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3624dd-6e6d-4b30-9b2c-5b33c3d02ce8",
   "metadata": {},
   "source": [
    "## Drawbacks of Gradient Descent\n",
    "\n",
    "1. **Local Minima**: For non-convex functions, gradient descent can get stuck in local minima and fail to find the global minimum.\n",
    "\n",
    "2. **Sensitivity to Learning Rate**: The learning rate $ \\alpha $ needs to be carefully chosen. A rate that's too large can overshoot the minimum, while a rate that's too small can make the algorithm slow to converge or get stuck.\n",
    "\n",
    "3. **Computational Cost**: Calculating the gradient can be computationally expensive for complex functions.\n",
    "\n",
    "4. **Numerical Errors**: For very flat or very steep regions, numerical errors can become significant.\n",
    "\n",
    "5. **Initialization Dependent**: The initial point $ x_0 $ can have a significant impact on the convergence of the algorithm, especially for non-convex functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5ffb1c",
   "metadata": {},
   "source": [
    "#### Stopping conditions\n",
    "- maxit, i.e. a predetermined maximum number of iterations. \n",
    "- abstol, i.e., stop when the function gets \"close enough\" to zero. \n",
    "- reltol, which is like your second suggestion, stop when the improvement drops below a threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270e6b74",
   "metadata": {},
   "source": [
    "## Python demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9bc0306a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numdifftools as nd\n",
    "import numpy as np\n",
    "from autograd import grad\n",
    "import numpy as np\n",
    "\n",
    "# Define objective function\n",
    "objective_function = lambda x: (x[0] ** 2) + (3 * (x[1] ** 2))\n",
    "\n",
    "# Initial point\n",
    "P0 = np.array([2.0, 1.0])\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.2\n",
    "\n",
    "# Stopping tolerance\n",
    "tol = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "156c2b43-c6ac-46e6-b080-22aaa2b89062",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient_descent(function, initial_point, learning_rate, tolerance):\n",
    "    point = initial_point\n",
    "    nb_iter = 0\n",
    "    old_loss = function(point)\n",
    "    gradient_function = grad(function)\n",
    "    \n",
    "    while True:\n",
    "        nb_iter += 1\n",
    "        gradient = np.array(gradient_function(point))\n",
    "        \n",
    "        # Update rule\n",
    "        point = point - (learning_rate * gradient)\n",
    "        \n",
    "        # Compute new loss\n",
    "        new_loss = function(point)\n",
    "\n",
    "        # Check for convergence\n",
    "        if abs(new_loss - old_loss) < tolerance:\n",
    "            print(\"Number of iterations:\", nb_iter)\n",
    "            break\n",
    "            \n",
    "        old_loss = new_loss\n",
    "\n",
    "    return point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "147935cf-fc59-483d-9443-3b8a0053d184",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 16\n",
      "The minimum occurs at: [5.64221981e-04 6.55360000e-12]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.183464443978558e-07"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run of the algorithm\n",
    "res = gradient_descent(objective_function, P0, lr, tol)\n",
    "print(\"The minimum occurs at:\", res)\n",
    "objective_function(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9388075f-cd99-4136-9edd-849673df2247",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 16\n",
      "3.183464443978558e-07\n",
      "Test 1 Passed!\n",
      "Number of iterations: 17\n",
      "4.0111651994129803e-07\n",
      "Test 2 Passed!\n",
      "Number of iterations: 17\n",
      "2.8651179995799273e-07\n",
      "Test 3 Passed!\n"
     ]
    }
   ],
   "source": [
    "def is_close(a, b, tol=1e-6):\n",
    "    print(np.abs(a - b))\n",
    "    return np.all(np.abs(a - b) < tol)\n",
    "\n",
    "# Test 1: Objective function f(x, y) = x^2 + 3y^2\n",
    "objective_function_2d = lambda x: (x[0] ** 2) + (3 * (x[1] ** 2))\n",
    "P0_2d = np.array([2.0, 1.0])\n",
    "lr = 0.2\n",
    "tol = 1e-6\n",
    "res_2d = gradient_descent(objective_function_2d, P0_2d, lr, tol)\n",
    "if is_close(objective_function_2d(res_2d), 0.0):\n",
    "    print(\"Test 1 Passed!\")\n",
    "else:\n",
    "    print(\"Test 1 Failed!\")\n",
    "    \n",
    "# Test 2: Objective function f(x, y, z) = x^2 + y^2 + z^2\n",
    "objective_function_3d = lambda x: x[0] ** 2 + x[1] ** 2 + x[2] ** 2\n",
    "P0_3d = np.array([2.0, 1.0, 3.0])\n",
    "res_3d = gradient_descent(objective_function_3d, P0_3d, lr, tol)\n",
    "if is_close(objective_function_3d(res_3d), 0.0):\n",
    "    print(\"Test 2 Passed!\")\n",
    "else:\n",
    "    print(\"Test 2 Failed!\")\n",
    "    \n",
    "# Test 3: Objective function f(x, y) = (x - 1)^2 + (y + 2)^2\n",
    "objective_function_shifted_2d = lambda x: (x[0] - 1) ** 2 + (x[1] + 2) ** 2\n",
    "P0_shifted_2d = np.array([2.0, 1.0])\n",
    "res_shifted_2d = gradient_descent(objective_function_shifted_2d, P0_shifted_2d, lr, tol)\n",
    "if is_close(objective_function_shifted_2d(res_shifted_2d), 0.0):\n",
    "    print(\"Test 3 Passed!\")\n",
    "else:\n",
    "    print(\"Test 3 Failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0359382f-4237-4967-a9ca-ac4c30b8308d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
