{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6d5ef08",
   "metadata": {},
   "source": [
    "# Newton-Raphson method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abfa78c",
   "metadata": {},
   "source": [
    "## La méthode de newton-raphson : Qu'est-ce que c'est ?\n",
    "\n",
    "L'algorithme de Newton, également connu sous le nom de méthode de Newton-Raphson, est une technique basique pour trouver rapidement et de manière itérative les racines réelles d'une fonction réelle. L'idée de base est d'utiliser la tangente à la courbe de la fonction pour estimer où la fonction elle-même coupe l'axe des x, c'est-à-dire là où la fonction s'annule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12c7314-7ccd-46de-95a4-0a22010513ae",
   "metadata": {},
   "source": [
    "## Quels usages ?\n",
    "\n",
    "La méthode de Newton est utilisée dans de nombreux domaines, y compris:\n",
    "\n",
    "- En sciences et en ingénierie pour résoudre des équations non linéaires.\n",
    "- En optimisation pour trouver les points où la dérivée première d'une fonction est nulle, c'est-à-dire les minima, les maxima et les points d'inflexion.\n",
    "- En finance pour évaluer des instruments financiers, comme les options, où l'équation de Black-Scholes peut être résolue pour la volatilité implicite.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7767bb7",
   "metadata": {},
   "source": [
    "## Avantages\n",
    "\n",
    "1. Convergence rapide: L'algorithme de Newton est très rapide quand il converge, surtout si la solution initiale est proche de la racine vraie.\n",
    "2. Efficacité: Il nécessite moins d'itérations par rapport à d'autres méthodes telles que la méthode de la bissection ou la méthode du point fixe.\n",
    "3. Précision: La méthode peut être très précise avec suffisamment d'itérations.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfb1301",
   "metadata": {},
   "source": [
    "## Inconvénients\n",
    "\n",
    "- Choix de l'initialisation: Le succès de la méthode dépend fortement de la valeur initiale. Un mauvais choix peut conduire à une convergence vers la mauvaise - racine ou à une divergence.\n",
    "- Nécessité de la dérivée: Il faut pouvoir calculer la dérivée première de la fonction, ce qui peut être complexe pour certaines fonctions.\n",
    "- Échec sur les points singuliers: Si la dérivée première est nulle au point de racine ou près de celle-ci, la méthode peut échouer ou avoir une convergence très lente.\n",
    "- Pas de garantie de convergence: Pour certaines fonctions, la méthode peut ne pas converger du tout.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ede478d",
   "metadata": {},
   "source": [
    "## Python demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c418c639-2bfa-462a-ad4c-f3c426a8ac0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trouvé la solution après 5 itérations.\n",
      "La racine est: 3.0000000149658455\n"
     ]
    }
   ],
   "source": [
    "def newton_method(f, df, x0, tol, max_iter):\n",
    "    \"\"\"\n",
    "    f : La fonction dont on cherche la racine.\n",
    "    df: La dérivée de la fonction f.\n",
    "    x0: Valeur initiale.\n",
    "    tol: La tolérance, qui détermine la précision de la solution.\n",
    "    max_iter: Le nombre maximal d'itérations.\n",
    "    \"\"\"\n",
    "    xn = x0\n",
    "    for n in range(0, max_iter):\n",
    "        fxn = f(xn)\n",
    "        if abs(fxn) < tol:\n",
    "            print('Trouvé la solution après', n, 'itérations.')\n",
    "            return xn\n",
    "        dfxn = df(xn)\n",
    "        if dfxn == 0:\n",
    "            print('La dérivée est nulle. Pas de solution trouvée.')\n",
    "            return None\n",
    "        xn = xn - fxn/dfxn\n",
    "    print('Exceedé le nombre maximal d\\'itérations. Pas de solution.')\n",
    "    return None\n",
    "\n",
    "# Exemple d'utilisation de la méthode de Newton:\n",
    "# On cherche la racine de f(x) = x^2 - 9\n",
    "\n",
    "f = lambda x: x**2 - 9\n",
    "df = lambda x: 2*x\n",
    "x0 = 10\n",
    "tol = 0.000001\n",
    "max_iter = 10\n",
    "\n",
    "root = newton_method(f, df, x0, tol, max_iter)\n",
    "print(\"La racine est:\", root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b216c08",
   "metadata": {},
   "source": [
    "ans cet exemple, la fonction f est \n",
    "x^2 − 9, dont on sait que la racine est ±3. La dérivée df est 2x. \n",
    "On commence avec une estimation initiale x0 de 10. La tolérance tol est fixée à une petite valeur pour une grande précision, et max_iter limite le nombre d'itérations pour éviter une boucle infinie dans le cas où la méthode ne converge pas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c85b7d",
   "metadata": {},
   "source": [
    "### Autres références\n",
    "\n",
    "Dans la famille des méthodes quasi-Newton pour l'optimisation, il existe plusieurs variantes et améliorations du BFGS qui sont utilisées pour résoudre différents types de problèmes. Voici quelques-unes de ces variantes :\n",
    "\n",
    "1. **L-BFGS (Limited Memory BFGS)**:\n",
    "   - Une variante conçue pour les problèmes de grande taille. L-BFGS ne stocke pas la matrice hessienne complète mais utilise une quantité limitée de la mémoire pour stocker les approximations de l'inverse de la hessienne, ce qui le rend particulièrement utile lorsque la dimension du problème est élevée.\n",
    "\n",
    "2. **BFGS-B (Bounded BFGS)**:\n",
    "   - Une modification du BFGS qui permet de gérer les contraintes de bornes sur les variables. C'est utile pour les problèmes d'optimisation avec des contraintes qui spécifient que la solution doit se trouver dans un intervalle donné.\n",
    "\n",
    "3. **DFP (Davidon-Fletcher-Powell)**:\n",
    "   - Un autre algorithme quasi-Newton qui utilise une formule de mise à jour différente pour la matrice hessienne inverse. Bien que moins populaire que BFGS en raison de sa convergence généralement plus lente, DFP a été l'un des premiers algorithmes de ce type et reste important d'un point de vue historique.\n",
    "\n",
    "4. **SR1 (Symmetric Rank 1)**:\n",
    "   - Une méthode de mise à jour de rang 1 qui garantit que la matrice hessienne inverse reste symétrique. Contrairement à BFGS ou DFP, la mise à jour SR1 n'est pas garantie de rester positive définie, mais elle peut être très efficace lorsque l'approximation de la hessienne requise est loin de la hessienne réelle.\n",
    "\n",
    "5. **Powell's symmetric BFGS (PSB)**:\n",
    "   - Une variation sur BFGS développée par Michael J.D. Powell qui maintient la symétrie de la matrice sans nécessiter qu'elle soit définie positive à chaque itération.\n",
    "\n",
    "6. **Hybrides et Variantes Adaptatives**:\n",
    "   - Certains algorithmes combinent des éléments de BFGS avec d'autres approches, comme les méthodes de gradient conjugué, pour essayer de capitaliser sur les forces de différentes méthodes.\n",
    "\n",
    "7. **Méthodes de quasi-Newton non linéaires**:\n",
    "   - Certains problèmes spécifiques peuvent nécessiter des adaptations de la formule de mise à jour quasi-Newton pour tenir compte de non-linéarités spécifiques ou de comportements de la fonction objectif."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d457aaf3-dad6-4935-8c0a-e0e59091d3cc",
   "metadata": {},
   "source": [
    "---\n",
    "## Sources\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
